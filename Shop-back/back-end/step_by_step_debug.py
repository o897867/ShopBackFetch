#!/usr/bin/env python3
"""
é€æ­¥è°ƒè¯•è„šæœ¬ - æ‰¾å‡ºæœåŠ¡å™¨å’Œæœ¬åœ°çš„å·®å¼‚
"""

def step1_basic_import():
    print("=== æ­¥éª¤1: åŸºæœ¬å¯¼å…¥æµ‹è¯• ===")
    try:
        import requests
        import sqlite3
        from bs4 import BeautifulSoup
        import re
        from datetime import datetime
        from dataclasses import dataclass
        print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def step2_network_test():
    print("\n=== æ­¥éª¤2: ç½‘ç»œè¿æ¥æµ‹è¯• ===")
    try:
        import requests
        response = requests.get('https://httpbin.org/get', timeout=10)
        print(f"âœ“ åŸºæœ¬ç½‘ç»œè¿æ¥æ­£å¸¸: {response.status_code}")
        
        response = requests.get('https://www.shopback.com.au/', timeout=10)
        print(f"âœ“ ShopBackä¸»é¡µè®¿é—®: {response.status_code}")
        return True
    except Exception as e:
        print(f"âŒ ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        return False

def step3_page_content_test():
    print("\n=== æ­¥éª¤3: é¡µé¢å†…å®¹æµ‹è¯• ===")
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get('https://www.shopback.com.au/agoda', headers=headers, timeout=30)
        
        print(f"çŠ¶æ€ç : {response.status_code}")
        print(f"å†…å®¹é•¿åº¦: {len(response.content)}")
        
        # æ£€æŸ¥å†…å®¹
        content = response.text.lower()
        checks = [
            ('åŒ…å«agoda', 'agoda' in content),
            ('åŒ…å«cashback', 'cashback' in content),
            ('åŒ…å«data-testid', 'data-testid' in content),
        ]
        
        all_good = True
        for name, result in checks:
            status = 'âœ“' if result else 'âŒ'
            print(f"{status} {name}")
            if not result:
                all_good = False
        
        return all_good
        
    except Exception as e:
        print(f"âŒ é¡µé¢å†…å®¹æµ‹è¯•å¤±è´¥: {e}")
        return False

def step4_parsing_test():
    print("\n=== æ­¥éª¤4: é¡µé¢è§£ææµ‹è¯• ===")
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get('https://www.shopback.com.au/agoda', headers=headers, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æµ‹è¯•å…³é”®å…ƒç´ è§£æ
        current_offer = soup.find(attrs={'data-testid': 'current-offer'})
        if current_offer:
            print(f"âœ“ current-offeræ‰¾åˆ°: {current_offer.get_text()}")
        else:
            print("âŒ current-offeræœªæ‰¾åˆ°")
            return False
            
        cashback_rates = soup.find('div', {'data-testid': 'cashback-rates'})
        if cashback_rates:
            print(f"âœ“ cashback-rateså®¹å™¨æ‰¾åˆ°")
            
            # æŸ¥æ‰¾è¯¦ç»†åˆ†ç±»
            rate_rows = cashback_rates.find_all('div', class_=lambda x: x and 'flex_row' in str(x))
            print(f"âœ“ æ‰¾åˆ° {len(rate_rows)} ä¸ªåˆ†ç±»è¡Œ")
            
            for i, row in enumerate(rate_rows[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                p_elements = row.find_all('p')
                if len(p_elements) >= 2:
                    category = p_elements[0].get_text().strip()
                    rate = p_elements[1].get_text().strip()
                    print(f"  åˆ†ç±»{i+1}: {category} -> {rate}")
        else:
            print("âŒ cashback-rateså®¹å™¨æœªæ‰¾åˆ°")
            return False
            
        print("âœ“ é¡µé¢è§£ææµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ é¡µé¢è§£ææµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def step5_scraper_test():
    print("\n=== æ­¥éª¤5: å®Œæ•´æŠ“å–å™¨æµ‹è¯• ===")
    try:
        from sb_scrap import ShopBackSQLiteScraper
        scraper = ShopBackSQLiteScraper('debug_test.db')
        
        result = scraper.scrape_store_page('https://www.shopback.com.au/agoda')
        
        print(f"æŠ“å–æˆåŠŸ: {result.scraping_success}")
        print(f"å•†å®¶åç§°: {result.name}")
        print(f"ä¸»è¦cashback: {result.main_cashback}")
        print(f"è¯¦ç»†åˆ†ç±»æ•°é‡: {len(result.detailed_rates)}")
        
        if result.error_message:
            print(f"é”™è¯¯ä¿¡æ¯: {result.error_message}")
            
        return result.scraping_success
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æŠ“å–å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("å¼€å§‹é€æ­¥è°ƒè¯•...")
    
    steps = [
        step1_basic_import,
        step2_network_test, 
        step3_page_content_test,
        step4_parsing_test,
        step5_scraper_test
    ]
    
    for i, step in enumerate(steps, 1):
        success = step()
        if not success:
            print(f"\nğŸ’¥ åœ¨æ­¥éª¤{i}å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
            break
        print(f"âœ… æ­¥éª¤{i}é€šè¿‡")
    else:
        print(f"\nğŸ‰ æ‰€æœ‰æ­¥éª¤éƒ½é€šè¿‡äº†ï¼é—®é¢˜å¯èƒ½åœ¨å…¶ä»–åœ°æ–¹")

if __name__ == "__main__":
    main()
